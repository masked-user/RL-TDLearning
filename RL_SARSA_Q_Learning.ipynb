{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bd0_WCe84ahF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "# --- 1. Environment Setup ---\n",
        "class CliffWalkingEnv:\n",
        "    def __init__(self, rows=4, cols=12):\n",
        "        self.rows, self.cols = rows, cols\n",
        "        self.start, self.goal = (3, 0), (3, 11)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.start\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        r, c = self.state\n",
        "        if action == 0: r = max(r - 1, 0)\n",
        "        elif action == 1: r = min(r + 1, self.rows - 1)\n",
        "        elif action == 2: c = max(c - 1, 0)\n",
        "        elif action == 3: c = min(c + 1, self.cols - 1)\n",
        "\n",
        "        next_state = (r, c)\n",
        "        if r == 3 and 1 <= c <= 10: # Falling off cliff\n",
        "            return self.start, -100, True\n",
        "        elif next_state == self.goal:\n",
        "            return next_state, 0, True\n",
        "        else:\n",
        "            self.state = next_state\n",
        "            return next_state, -1, False\n",
        "\n",
        "    def get_action_space(self):\n",
        "        return [0, 1, 2, 3]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Helper Functions ---\n",
        "def epsilon_greedy(Q, state, actions, epsilon):\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice(actions)\n",
        "    q_vals = Q[state]\n",
        "    max_q = np.max(q_vals)\n",
        "    best_actions = [a for a in actions if q_vals[a] == max_q]\n",
        "    return random.choice(best_actions)\n",
        "\n",
        "def smooth(x, window=10):\n",
        "    return np.convolve(x, np.ones(window)/window, mode='valid')"
      ],
      "metadata": {
        "id": "MZl3gQIw4yX4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sarsa(env, episodes=500, alpha=0.5, gamma=1.0, epsilon=0.1):\n",
        "    Q = defaultdict(lambda: np.zeros(4))\n",
        "    rewards = []\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state = env.reset()\n",
        "        # SARSA picks the FIRST action before the loop starts\n",
        "        action = epsilon_greedy(Q, state, env.get_action_space(), epsilon)\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            next_state, reward, done = env.step(action)\n",
        "\n",
        "            # SARSA picks the NEXT action now to use it in the update\n",
        "            next_action = epsilon_greedy(Q, next_state, env.get_action_space(), epsilon)\n",
        "\n",
        "            # Update uses the Q-value of the action we ARE going to take\n",
        "            TD_target = reward + gamma * Q[next_state][next_action]\n",
        "            Q[state][action] += alpha * (TD_target - Q[state][action])\n",
        "\n",
        "            state, action = next_state, next_action\n",
        "            total_reward += reward\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "    return Q, rewards"
      ],
      "metadata": {
        "id": "CHFl53HZ4xqi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning(env, episodes=500, alpha=0.5, gamma=1.0, epsilon=0.1):\n",
        "    Q = defaultdict(lambda: np.zeros(4))\n",
        "    rewards = []\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Q-Learning picks the current action\n",
        "            action = epsilon_greedy(Q, state, env.get_action_space(), epsilon)\n",
        "            next_state, reward, done = env.step(action)\n",
        "\n",
        "            # Update uses the MAX Q-value (best case), ignoring epsilon-greedy\n",
        "            best_next_action = np.argmax(Q[next_state])\n",
        "            TD_target = reward + gamma * Q[next_state][best_next_action]\n",
        "            Q[state][action] += alpha * (TD_target - Q[state][action])\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "    return Q, rewards"
      ],
      "metadata": {
        "id": "dolm7pT55Ek9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Execution & Comparison ---\n",
        "\n",
        "env = CliffWalkingEnv()\n",
        "Q_sarsa, r_sarsa = sarsa(env)\n",
        "Q_ql, r_ql = q_learning(env)\n",
        "\n"
      ],
      "metadata": {
        "id": "KLbf07No4uJT"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(smooth(r_sarsa), label='SARSA (Safer)', color='blue')\n",
        "plt.plot(smooth(r_ql), label='Q-Learning (Riskier)', color='orange')\n",
        "plt.axhline(y=-13, color='r', linestyle='--', label='Optimal Reward (-13)')\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Sum of Rewards during Episode')\n",
        "plt.title('SARSA vs Q-Learning Performance')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GnrEV_6O5wmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 5. Path Visualization ---\n",
        "def visualize_path(Q, env, name):\n",
        "    print(f\"\\n--- {name} Path Visualization ---\")\n",
        "    grid = np.full((env.rows, env.cols), '.', dtype=str)\n",
        "    state = env.start\n",
        "    grid[env.goal] = 'G'\n",
        "    grid[env.start] = 'S'\n",
        "    grid[3, 1:11] = 'C' # Mark the Cliff\n",
        "\n",
        "    path_steps = 0\n",
        "    while state != env.goal and path_steps < 50:\n",
        "        action = np.argmax(Q[state])\n",
        "        r, c = state\n",
        "        if action == 0: r -= 1\n",
        "        elif action == 1: r += 1\n",
        "        elif action == 2: c -= 1\n",
        "        elif action == 3: c += 1\n",
        "\n",
        "        state = (max(0, min(r, env.rows-1)), max(0, min(c, env.cols-1)))\n",
        "        if grid[state] == '.': grid[state] = '*'\n",
        "        path_steps += 1\n",
        "\n",
        "    for row in grid:\n",
        "        print(' '.join(row))\n",
        "\n",
        "env_viz = CliffWalkingEnv()\n",
        "visualize_path(Q_sarsa, env_viz, \"SARSA\")\n",
        "visualize_path(Q_ql, env_viz, \"Q-Learning\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1S8HDlC4rdd",
        "outputId": "bbc62b61-78a4-45c8-91e1-c5fb9c25130d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- SARSA Path Visualization ---\n",
            "* * * * * * * * * * . .\n",
            "* . . . . . . . . * * .\n",
            "* . . . . . . . . . * *\n",
            "S C C C C C C C C C C G\n",
            "\n",
            "--- Q-Learning Path Visualization ---\n",
            ". . . . . . . . . . . .\n",
            ". . . . . . . . . . . .\n",
            "* * * * * * * * * * * *\n",
            "S C C C C C C C C C C G\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W01L8X0z44Bt"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}